version: '3.8'

services:
  ollama-service:
    image: ollama/ollama:latest
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    volumes:
      - ./ollama-data:/root/.ollama
    ports:
      - "11434:11434"
    command: ["/bin/sh", "-c", "ollama pull llama3 && ollama serve"]
    networks:
      - vision_llm_net

  yolo-service:
    build:
      context: ./yolo-app
      dockerfile: Dockerfile
    container_name: yolo_vision
    runtime: nvidia
    ipc: host
    devices:
      - /dev/video0:/dev/video0
    environment:
      - DISPLAY=${DISPLAY}
      - OLLAMA_HOST=http://ollama-service:11434
    volumes:
      - /tmp/.X11-unix:/tmp/.X11-unix
    depends_on:
      - ollama-service
    networks:
      - vision_llm_net

networks:
  vision_llm_net:
    driver: bridge
